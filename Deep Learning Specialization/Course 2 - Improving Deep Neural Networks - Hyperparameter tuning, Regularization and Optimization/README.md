# Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization 

## Description
After 3 weeks, you will: - Understand industry best-practices for building deep learning applications. - Be able to effectively use the common neural network "tricks", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, - Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. - Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance - Be able to implement a neural network in TensorFlow. This is the second course of the Deep Learning Specialization.

- #### Week 1: Practical aspects of Deep Learning
	- Give examples of how different types of initializations can lead to different results
	- Examine the importance of initialization in complex neural networks
	- Explain the difference between train/dev/test sets
	- Diagnose the bias and variance issues in your model
	- Assess the right time and place for using regularization methods such as dropout or L2 regularization
	- Explain Vanishing and Exploding gradients and how to deal with them
	- Use gradient checking to verify the accuracy of your backpropagation implementation 
- #### Week 2: Optimization algorithms
	- Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
	- Use random minibatches to accelerate convergence and improve optimization
	- Describe the benefits of learning rate decay and apply it to your optimization
- #### Week 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks
	- Master the process of hyperparameter tunin

---

# Certification
<p align="center">
  <img src="../Deep Learning Certification Images/Courses/Improving_DNM_Hyperparameter_tuning_Regularization_and_Optimization.jpg" | width=800 />
</p>
