# Course 2: 
- #### Week 1: Practical aspects of Deep Learning
	- Give examples of how different types of initializations can lead to different results
	- Examine the importance of initialization in complex neural networks
	- Explain the difference between train/dev/test sets
	- Diagnose the bias and variance issues in your model
	- Assess the right time and place for using regularization methods such as dropout or L2 regularization
	- Explain Vanishing and Exploding gradients and how to deal with them
	- Use gradient checking to verify the accuracy of your backpropagation implementation 
- #### Week 2: Optimization algorithms
	- Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
	- Use random minibatches to accelerate convergence and improve optimization
	- Describe the benefits of learning rate decay and apply it to your optimization
- #### Week 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks
	- Master the process of hyperparameter tunin

---

# Certification
<p align="center">
  <img src="../Deep Learning Certification Images/Courses/Improving_DNM_Hyperparameter_tuning_Regularization_and_Optimization.jpg" | width=800 />
</p>
