# Course 2: NLP with Probabilistic Models

## Description
In Course 2 of the NLP Specialization, offered by deeplearning.ai, you will: create a simple auto-correct algorithm using minimum edit distance and dynamic programming, apply the Viterbi Algorithm for part-of-speech (POS) tagging, which is important for computational linguistics, write a better auto-complete algorithm using an N-gram language model, and write your own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model.

- #### Week 1: Autocorrect
	- Learn about autocorrect, minimum edit distance, and dynamic programming, then build your own spellchecker to correct misspelled words!
- #### Week 2: Part of Speech Tagging and Hidden Markov Models
	- Learn about Markov chains and Hidden Markov models, then use them to create part-of-speech tags for a Wall Street Journal text corpus!
- #### Week 3: Autocomplete and Language Models
	- Learn about how N-gram language models work by calculating sequence probabilities, then build your own autocomplete language model using a text corpus from Twitter!
- #### Week 4: Word embeddings with neural networks
	- Learn about how word embeddings carry the semantic meaning of words, which makes them much more powerful for NLP tasks, then build your own Continuous bag-of-words model to create word embeddings from Shakespeare text.

---

# Certification
<p align="center">
  <img src="../Natural Language Processing Certification Images/Courses/Natural_Language_Processing_with_Probabilistic.jpg" | width=800 />
</p>
